{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d094c11-0067-4c53-b6e5-dd4a6d43f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/user_data/xianjunxing/.conda/envs/python_tda/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/share/user_data/xianjunxing/.conda/envs/python_tda/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/share/user_data/xianjunxing/.conda/envs/python_tda/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/share/user_data/xianjunxing/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/share/user_data/xianjunxing/.conda/envs/python_tda/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "2024-11-15 19:12:58.278617: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-15 19:12:58.328698: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-15 19:12:58.329188: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 19:12:59.134777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('./tools/')\n",
    "from AlignFilter import *\n",
    "from LocalGeo import *\n",
    "from MapperTools import *\n",
    "from DataProcess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240d4bc4-71ac-4ec4-a55b-56bb5f1b9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STIM step-1\n",
    "## generate a shared low-dimensional embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fa2e9b-c7ef-4ebb-b9b4-b6558dc6f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STIM_LowDimSpace(clip_ts_ls,model):\n",
    "    \"\"\"\n",
    "    Function to transform a list of time-series (TS) data into a low-dimensional space using UMAP.\n",
    "    \n",
    "    Parameters:\n",
    "    clip_ts_ls: list of numpy.ndarray. each element:n_subj * ROIs * nTR time-series data arrays.\n",
    "    **kwargs: Additional keyword arguments to be passed to the align_umap function.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: low-dimensional embeddings for the input time-series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_subs = clip_ts_ls[0].shape[0]  # Number of subjects (assumed to be the same across all clips)\n",
    "    \n",
    "    # Normalize the time-series data along the axis representing ROIs (regions of interest)\n",
    "    ts_norm = []  # Initialize an empty list to store normalized time-series\n",
    "    for ts_i in clip_ts_ls:\n",
    "        # Scale each subject's time-series data along the ROI axis\n",
    "        ts_i_norm = [scale(ts_i[i], axis=1) for i in range(n_subs)]\n",
    "        ts_norm.append(np.array(ts_i_norm))  # Append the normalized data for each clip/movie\n",
    "    # Select indices of movies/clips longer than 2 minutes and concatenate their time-series data\n",
    "    ts_arr = np.dstack([ts_i for ts_i in ts_norm])  # Stack the selected clips along a new axis\n",
    "    print('time-series shape =',ts_arr.shape)  # Print the shape of the concatenated time-series data\n",
    "    \n",
    "    # Transform the time-series data into 3-dimensional UMAP embeddings\n",
    "    # Note: This step is computationally intensive and may take a long time to run\n",
    "    sub_embeds = align_projection(ts_arr=ts_arr,model=model)\n",
    "    print('embeddings shape =',sub_embeds.shape)  # Print the shape of the resulting UMAP embeddings\n",
    "    \n",
    "    return sub_embeds  # Return the UMAP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a86507-60b6-4271-a91c-82c1535723cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STIM step-2\n",
    "## generate individual-group shape graph, compute intersection TCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7dec09-31e1-425b-ac06-9af6fec51bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STIM_individual_mapper(sub_embeds, group_embeds=None, cover_n=12, cover_overlap=0.5, eps=0.7):\n",
    "    \"\"\"\n",
    "    Function to map individual subject UMAP embeddings to a group-level UMAP and calculate sub-matrices of the Temporal Connectivity Matrix (TCM).\n",
    "    \n",
    "    Parameters:\n",
    "    sub_embeds (numpy.ndarray): UMAP embeddings for individual subjects, shape (n_sub, n_TR, n_components).\n",
    "    group_umap (numpy.ndarray or None): Optional group-level UMAP embeddings. If None, the mean of sub_umap is used. Default is None.\n",
    "    cover_n (int): Number of intervals in the cover (resolution parameter for Mapper). Default is 12.\n",
    "    cover_overlap (float): Fractional overlap of intervals in the cover. Default is 0.5.\n",
    "    eps (float): Epsilon parameter for DBSCAN clustering. Default is 0.7.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of sub-matrices of the Temporal Connectivity Matrix (TCM) for each subject.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If group_umap is not provided, calculate it as the mean of individual subject embeddings\n",
    "    if group_embeds is None:\n",
    "        group_embeds = np.mean(sub_embeds, axis=0)\n",
    "    \n",
    "    # Normalize the group UMAP embeddings along the features (columns)\n",
    "    group_embeds_scale = scale(group_embeds, axis=0)\n",
    "    \n",
    "    sub_score = []  # Initialize an empty list to store scores (not used in this code)\n",
    "    sub_n = np.array(sub_embeds).shape[0]  # Number of subjects\n",
    "    \n",
    "    # Normalize each subject's UMAP embeddings\n",
    "    sub_embeds_scale = [scale(sub_embeds[i], axis=0) for i in range(sub_n)]\n",
    "    \n",
    "    # Combine all subjects' embeddings and the group embedding into a single matrix\n",
    "    embeds = np.vstack([sub_embeds_scale[i] for i in range(sub_n)] + [group_embeds_scale])\n",
    "    g_size = int(group_embeds_scale.shape[0])  # Size of the group embedding (number of time points)\n",
    "    \n",
    "    # Initialize KeplerMapper\n",
    "    mapper = KeplerMapper(verbose=0)\n",
    "    \n",
    "    # Fit the Mapper to the combined embeddings and project onto the first three components\n",
    "    lens = mapper.fit_transform(embeds, projection=[0, 1, 2])\n",
    "    \n",
    "    # Extract the lens (projection) corresponding to the group-level UMAP\n",
    "    group_lens = lens[g_size * sub_n:, :]\n",
    "    group_tcm = []  # Initialize an empty list to store the group TCM (not used in this code)\n",
    "\n",
    "    args_list = []  # Initialize a list to store arguments for parallel processing\n",
    "    \n",
    "    # Prepare the arguments for each subject for parallel processing\n",
    "    for i in tqdm(range(sub_n)):\n",
    "        # Extract the lens for the current subject\n",
    "        sub_lens = lens[g_size * i:g_size * (i + 1), :]\n",
    "        \n",
    "        # Combine the group lens with the current subject lens\n",
    "        lens_i = np.vstack([group_lens, sub_lens])\n",
    "        \n",
    "        # Combine the group embedding with the current subject embedding\n",
    "        embeds_i = np.vstack([group_embeds_scale, sub_embeds_scale[i]])\n",
    "        \n",
    "        # Append the arguments to the list\n",
    "        args_list.append([embeds_i, g_size, cover_n, cover_overlap, lens_i, eps])\n",
    "\n",
    "    # Initialize a multiprocessing pool with the number of available CPU cores\n",
    "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "    \n",
    "    # Process each subject in parallel to compute the sub-matrices of the TCM\n",
    "    results = pool.map(process_sub_embeds, args_list)\n",
    "    \n",
    "    # Close the pool and wait for all processes to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return results  # Return the list of sub-matrices of the TCM for each subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602492ef-1ca2-4361-afa0-844509a1d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STIM step-3\n",
    "## extract global topology and local geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16f79dc-1ac3-4c48-8738-f4a254ec6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STIM_global_topology(indi_TCM):\n",
    "    \"\"\"\n",
    "    Function to extract the global topology from individual Temporal Connectivity Matrices (TCM).\n",
    "    \n",
    "    Parameters:\n",
    "    indi_TCM (numpy.ndarray): A 3D array of shape (n_subj, nTR, nTR) representing individual TCMs,\n",
    "                              where n_subj is the number of subjects, and nTR is the number of time points.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: A 2D array where each row contains the diagonal elements of the corresponding TCM,\n",
    "                   representing the global topology for each subject.\n",
    "    \"\"\"\n",
    "    # Extract the diagonal elements (global topology) from each individual's TCM\n",
    "    glob_topo = np.array([_.diagonal() for _ in indi_TCM])\n",
    "    \n",
    "    return glob_topo  # Return the global topology for all subjects\n",
    "\n",
    "def STIM_local_geometry(indi_TCM, template_TCM):\n",
    "    \"\"\"\n",
    "    Function to analyze the local geometry of individual TCMs relative to a template TCM.\n",
    "    \n",
    "    Parameters:\n",
    "    indi_TCM (numpy.ndarray): A 3D array of shape (n_subj, nTR, nTR) representing individual TCMs.\n",
    "    template_TCM (numpy.ndarray): A 3D array of shape (template_subj, nTR, nTR) representing the template TCM,\n",
    "                                  where template_subj is the number of template subjects.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: A 2D array where each row contains the weighted clique scores for each segment (scene)\n",
    "                   identified in the TCMs, representing the local geometry for each subject.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the mean TCM across template subjects and normalize it\n",
    "    template_TCM = template_TCM / max(template_TCM.flatten())  # Normalize the mean TCM by its maximum value\n",
    "    \n",
    "    # Find the best segmentation (scene transitions) in the mean TCM using a penalty search\n",
    "    best_scene_tr, performance = search_best_param(template_TCM, method='tr_level', penalty=range(2, 60))\n",
    "    \n",
    "    clique_score = []  # Initialize a list to store the clique scores for each segment\n",
    "    \n",
    "    # Iterate over each identified scene transition (segment)\n",
    "    for tr_i in best_scene_tr:\n",
    "        a, b = tr_i  # Extract the start and end time points of the segment\n",
    "        \n",
    "        # Extract the corresponding submatrix (clique) from the individual TCMs\n",
    "        clique_mtx = indi_TCM[:, a:b, a:b]\n",
    "        \n",
    "        # Symmetrize each clique by averaging it with its transpose\n",
    "        clique_mtx_symm = np.array([(_.T + _) / 2 for _ in clique_mtx])\n",
    "        \n",
    "        # Calculate the weighted clique score for each symmetrized submatrix\n",
    "        clique_score.append(weighted_clique_score(clique_mtx_symm))\n",
    "    \n",
    "    clique_score = np.array(clique_score)  # Convert the list of clique scores to a numpy array\n",
    "    \n",
    "    return clique_score,best_scene_tr  # Return the local geometry (clique scores) for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd3b2328-788b-433e-a3ea-58b202d28968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate shape graph\n",
    "from kmapper import KeplerMapper, Cover\n",
    "from sklearn.cluster import DBSCAN\n",
    "from dyneusr import DyNeuGraph\n",
    "\n",
    "def g_mapper_graph(low_d_embeds,cover_n=20,cover_overlap=0.4,eps=0.5):\n",
    "    embeds = low_d_embeds\n",
    "    mapper = KeplerMapper(verbose=0)\n",
    "    lens = mapper.fit_transform(embeds,projection=[0,1,2])\n",
    "    graph = mapper.map(lens, X=embeds, cover=Cover(cover_n, cover_overlap, limits=np.array([[0,1],[0,1],[0,1]])),clusterer=DBSCAN(eps=eps), )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8027c58c-a54b-405c-a9e6-618f6ee337ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit to your data path\n",
    "hcp_18clips_ts = load_npz('../data/Fig2/ts/hcp_18clips_ts_l5r5.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4903a6-f809-4c3f-bb76-e58bff67d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove clips with less than two minutes of data\n",
    "hcp_13clips_ts = []\n",
    "movie_ind = [0, 1, 2, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16]\n",
    "for i in movie_ind:\n",
    "    hcp_13clips_ts.append(hcp_18clips_ts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cde317a-f3b2-4b6b-8a1c-0de8f5a264f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-series shape = (170, 271, 2804)\n",
      "embeddings shape = (170, 2804, 3)\n"
     ]
    }
   ],
   "source": [
    "# Select a filter (here, we use UMAP)\n",
    "umap_model = UMAP(n_components=3,n_neighbors=25,random_state=42,min_dist=0.01,metric='euclidean')\n",
    "\n",
    "# Apply STIM to the data\n",
    "hcp_13clips_umap = STIM_LowDimSpace(hcp_13clips_ts,umap_model)\n",
    "\n",
    "# Compute group consensus\n",
    "group_umap = hcp_13clips_umap.mean(axis=0)\n",
    "graph_group = g_mapper_graph(low_d_embeds=scale(group_umap, axis=0), cover_n=12, cover_overlap=0.5, eps=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7183bb3-fe28-4f96-b24e-fc4f77a5e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute group consensus\n",
    "group_umap = hcp_13clips_umap.mean(axis=0)\n",
    "graph_group = g_mapper_graph(low_d_embeds=scale(group_umap, axis=0), cover_n=12, cover_overlap=0.5, eps=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b34b5e-aa40-466f-bc70-5bfc7e3e979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize shape graph\n",
    "dG = DyNeuGraph(G=graph_group)\n",
    "dG.visualize('your_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803e9dd6-2f4a-41c9-96ea-1e732b75322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:00<00:00, 23907.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 2804, 2804)\n"
     ]
    }
   ],
   "source": [
    "hcp_170subs_tcm = STIM_individual_mapper(hcp_13clips_umap)\n",
    "print(np.array(hcp_170subs_tcm).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21e19ab-e6e9-47e8-879c-1e09629c9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 2804)\n"
     ]
    }
   ],
   "source": [
    "hcp_glob_topo = STIM_global_topology(hcp_170subs_tcm)\n",
    "print(hcp_glob_topo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ffbe25-47b1-4a8c-a937-be036ffb7d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:04<00:00, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 170)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hcp_tcm_mov1 = np.array(hcp_170subs_tcm)[:,:243,:243]\n",
    "hcp_local_geo, clip_events = STIM_local_geometry(indi_TCM=hcp_tcm_mov1,template_TCM=hcp_tcm_mov1.mean(axis=0))\n",
    "print(hcp_local_geo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87dd3ff2-fa15-4c85-876d-ce5d253a7705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 35),\n",
       " (36, 58),\n",
       " (59, 79),\n",
       " (80, 120),\n",
       " (121, 130),\n",
       " (131, 152),\n",
       " (153, 164),\n",
       " (165, 181),\n",
       " (182, 242)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce6581-17e4-466c-8a14-66b974055db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_tda",
   "language": "python",
   "name": "python_tda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
